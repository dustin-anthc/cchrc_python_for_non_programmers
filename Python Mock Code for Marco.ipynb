{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Code for BEES Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to enter filename for enstar data here\n",
    "enstar_data = pd.read_csv()\n",
    "\n",
    "bees_records = pd.read_csv(\"bees_records_with_GoogleMapsAPI_standardized_addresses.csv\")\n",
    "\n",
    "matsu_property = pd.read_csv(\"matsu_borough_residential_property_tax_data.csv\")\n",
    "\n",
    "anc_com_property = pd.read_csv(\"anchorage_property_tax_data_commercial_housing_side.csv\")\n",
    "\n",
    "anc_res_property = pd.read_csv(\"anchorage_property_tax_data_residential_side.csv\")\n",
    "\n",
    "kenai_property = pd.read_csv(\"kenai_peninsula_borough_property_tax_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enstar, BEES, Property Tax datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I don't have the column headers for the Enstar data, so this will need to be added\n",
    "enstar_bees = pd.merge(enstar_data, bees_records, left_on='enter_enstar_address_field_here', \n",
    "                         right_on='final_address', how='left')\n",
    "\n",
    "\n",
    "# The final_site_address field is the one I gave for you guys to convert using the GoogleMapsAPI Geocoder.\n",
    "# If you created a new column name for the standardized address, it should be entered instead.\n",
    "# Also, let me know if you need me to convert these addresses using the Geocoder.\n",
    "final_dataset = pd.merge(enstar_bees, matsu_property, left_on='enter_enstar_address_field_here',\n",
    "                        right_on='final_site_address', how='left')\n",
    "\n",
    "\n",
    "# The final_site_address field is the one I gave for you guys to convert using the GoogleMapsAPI Geocoder.\n",
    "# If you created a new column name for the standardized address, it should be entered instead.\n",
    "# Also, let me know if you need me to convert these addresses using the Geocoder.\n",
    "final_dataset = pd.merge(final_dataset, anc_com_property, left_on='enter_enstar_address_field_here',\n",
    "                        right_on='final_site_address', how='left')\n",
    "\n",
    "\n",
    "# The final_site_address field is the one I gave for you guys to convert using the GoogleMapsAPI Geocoder.\n",
    "# If you created a new column name for the standardized address, it should be entered instead.\n",
    "# Also, let me know if you need me to convert these addresses using the Geocoder.\n",
    "final_dataset = pd.merge(final_dataset, anc_res_property, left_on='enter_enstar_address_field_here',\n",
    "                        right_on='final_site_address', how='left')\n",
    "\n",
    "\n",
    "# The final_site_address field is the one I gave for you guys to convert using the GoogleMapsAPI Geocoder.\n",
    "# If you created a new column name for the standardized address, it should be entered instead.\n",
    "# Also, let me know if you need me to convert these addresses using the Geocoder.\n",
    "final_dataset = pd.merge(final_dataset, kenai_property, left_on='enter_enstar_address_field_here',\n",
    "                        right_on='final_site_address', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heating Degree Day Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I put these folders in the Google Drive folder \"shared_datasets_with_marco\"\n",
    "# We will need this data to be able to normalize for the climate in different \n",
    "# years. \n",
    "\n",
    "# This dataset allows us to look up the best weather station for heating degree\n",
    "# data based on the zip code.\n",
    "zipcode_to_weather_station = pd.read_csv(\"all_alaska_zipcode_to_station_lookups.csv\")\n",
    "\n",
    "# You will need to create a zip code column from the address field in the final dataset\n",
    "final_dataset_with_zips = pd.merge(final_dataset, zipcode_to_weather_station, how='left',\n",
    "                                      left_on='create_zipcode_field_here', right_on='zip_code')\n",
    "\n",
    "# This dataset has the monthly heating degree days from all of the reliable \n",
    "# weather stations in Alaska that have data going back 10 years.  We may need\n",
    "# to get some additional data that goes back farther in time.\n",
    "hdd_data = pd.read_csv(\"all_alaska_stations_base_60and65_hdd.csv\")\n",
    "\n",
    "# This will merge the correct heating degree day data for each zip code\n",
    "# using the proper weather station, year, and month.  You will have to \n",
    "# create two columns from the ENSTAR timestamp data for this to work:\n",
    "# the 'year_field' and the 'month_field'\n",
    "final_dataset_with_hdds = pd.merge(final_dataset_with_zips, hdd_data, how='left',\n",
    "                                  left_on=['station_name', 'year_field', 'month_field'], \n",
    "                                   right_on=['station_name', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' The next step is to take the monthly energy consumptions and divide it by the \n",
    "monthly_base_60_hdds. I would probably first multiply the ccf of natural gas by 100,000 to convert \n",
    "it to BTUs.  After that we should normalize by the square footage of the building, taken from the\n",
    "AkWarm BEES ratings or from the property tax databases. The final measure we will have is sometimes \n",
    "known as the \"Home Heating Index\", and the units will be BTUs/HDD/square foot.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heatpump_env",
   "language": "python",
   "name": "heatpump_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
